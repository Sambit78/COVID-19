#install.packages("trendyy")
#remotes::install_github("josiahparry/gtrendsR", "interest_refactor")
library(trendyy)
library(dplyr)
library(ggplot2)
# http://josiahparry.com/post/2019-05-25-introducing-trendyy/
# 1. Create Vector for search terms ----
#analytics <- c("paithani saree","banarasi saree","kantha saree","sambalpuri saree","khandua saree")
#analytics <- c("pasapali","bomkai","khandua silk")
#analytics  <- c("virat kohli","steve smith")
analytics <- c("peopleanalytics")
# 2. Get Query ----
analytics_trends <- trendy(analytics, from = "2015-04-01", to = Sys.Date())
# 3. Get Summary of trends ----
analytics_trends
# 4. Get Trend Data
get_interest(analytics_trends)
get_interest_country(analytics_trends)
# 5. Plot Trends
analytics_trends %>%
get_interest   %>%
ggplot(aes(date, hits, color = keyword)) +
#  geom_line() +
geom_smooth(span=0.5,se=FALSE)+
geom_point(alpha = .2) +
theme_minimal() +
theme(legend.position = "bottom") +
theme(plot.title = element_text(hjust = 0.5))+
labs(x = "",
y = "Relative Search Popularity",
title = "Google Search Popularity")
#  + ylim(0,40)
# 6. Get Related Queries
analytics_trends %>%
get_related_queries() %>%
group_by(keyword) %>%
sample_n(20)
#https://www.salvationarmy.org.au/about-us/news-and-stories/media-newsroom/2014-doorknock-result/
#install.packages("trendyy")
#remotes::install_github("josiahparry/gtrendsR", "interest_refactor")
library(trendyy)
library(dplyr)
library(ggplot2)
# http://josiahparry.com/post/2019-05-25-introducing-trendyy/
# 1. Create Vector for search terms ----
#analytics <- c("paithani saree","banarasi saree","kantha saree","sambalpuri saree","khandua saree")
#analytics <- c("pasapali","bomkai","khandua silk")
#analytics  <- c("virat kohli","steve smith")
analytics <- c("people analytics")
# 2. Get Query ----
analytics_trends <- trendy(analytics, from = "2015-04-01", to = Sys.Date())
# 3. Get Summary of trends ----
analytics_trends
# 4. Get Trend Data
get_interest(analytics_trends)
get_interest_country(analytics_trends)
# 5. Plot Trends
analytics_trends %>%
get_interest   %>%
ggplot(aes(date, hits, color = keyword)) +
#  geom_line() +
geom_smooth(span=0.5,se=FALSE)+
geom_point(alpha = .2) +
theme_minimal() +
theme(legend.position = "bottom") +
theme(plot.title = element_text(hjust = 0.5))+
labs(x = "",
y = "Relative Search Popularity",
title = "Google Search Popularity")
#  + ylim(0,40)
# 6. Get Related Queries
analytics_trends %>%
get_related_queries() %>%
group_by(keyword) %>%
sample_n(20)
#https://www.salvationarmy.org.au/about-us/news-and-stories/media-newsroom/2014-doorknock-result/
setwd("~/Google Drive/Data Analytics/NorthWestern/MSDS 422/People-Analytics---Predicting-employee-performance")
library(tidyverse)
library(psych)
library(QuantPsyc)
library(likert)
library(Hmisc)
survey <- read.table(("Customer sat N2507.csv"), header = T, sep = ",")
attach(survey)
install.packages("likert")
install.packages("Hmisc")
library(tidyverse)
library(psych)
library(QuantPsyc)
library(likert)
library(Hmisc)
survey <- read.table(("Customer sat N2507.csv"), header = T, sep = ",")
attach(survey)
setwd("~/Google Drive/Data Analytics/NorthWestern/MSDS 422/People-Analytics---Predicting-employee-performance")
library(tidyverse)
library(psych)
library(QuantPsyc)
library(likert)
library(Hmisc)
survey <- read.table(("Customer sat N2507.csv"), header = T, sep = ",")
attach(survey)
survey <- read.table(("Data Files\Customer sat N2507.csv"), header = T, sep = ",")
attach(survey)
setwd("~/Google Drive/Data Analytics/NorthWestern/MSDS 422/People-Analytics---Predicting-employee-performance/Data files")
setwd("~/Google Drive/Data Analytics/NorthWestern/MSDS 422/People-Analytics---Predicting-employee-performance/Data files")
library(tidyverse)
library(psych)
library(QuantPsyc)
library(likert)
library(Hmisc)
survey <- read.table(("Customer sat N2507.csv"), header = T, sep = ",")
attach(survey)
View(survey)
str(survey)
names(survey)
nrow(survey)
ncol(survey)
fix(survey)
library(QuantPsyc)
library(psych)
fix(survey)
likert_sat <- survey[1:4]
likert_loyalty <- survey[6]
likert_invest <- survey[7]
my_list <- list(likert_sat, likert_loyalty, likert_invest)
View(my_list)
View(likert_sat)
View(survey)
View(survey)
# Response frequencies
responses_fr <- lapply(my_list, response.frequencies)
responses_fr
likert_sat <- likert_sat %>%
mutate_if(is.numeric, as.factor)
summary(likert_data)
likert_data_results <- likert(likert_sat)
plot(likert_data_results, group.order = c("Sat1", "Sat2", "Sat3", "Sat4"))
# Total number of complete cases
data_complete <- nrow(na.omit(likert_sat))
# Total number of incomplete cases by variable
colSums(is.na(likert_sat))
data_missing <- sum(colSums(is.na(likert_sat))) # Total number
data_summary <- cbind(data_complete, data_missing)
rownames(data_summary) <- paste("Survey reponses",sep = " ")
data_summary
likert_sat <- likert_sat %>%
mutate_if(is.factor, as.numeric)
cronbachs_alpha <- alpha(likert_sat)
cronbachs_alpha_summary <- cronbachs_alpha$item.stats
cronbachs_alpha_r <- round(cronbachs_alpha$total$raw_alpha, 3)
cronbachs_alpha_r # No
# Cronbach's alpha if item drop
cronbachs_alpha_drop <- round(cronbachs_alpha$alpha.drop$raw_alpha, 3)
cronbachs_alpha_drop
cronbachs_alpha_drop_summary <- ifelse(cronbachs_alpha_drop >
cronbachs_alpha_r, "BAD", "OK")
cronbachs_alpha_drop_summary # Note. None of the items yield a higher alpha value if we drop them :D
# Cronbach's alpha for Corrected - Item Total Correlation
cronbachs_alpha_corrected <- round(cronbachs_alpha$item.stats$r.drop, 3)
cronbachs_alpha_corrected
cronbachs_alpha_corrected_summary <- ifelse(cronbachs_alpha_corrected <
0.3, "BAD", "OK")
cronbachs_alpha_corrected_summary # Note. None of items present a a correlation lower than .3 :D
likert_loyalty <- likert_loyalty %>%
mutate_if(is.numeric, as.factor)
summary(likert_loyalty)
likert_loyalty_results <- likert(likert_loyalty)
plot(likert_loyalty_results)
data_complete <- nrow(na.omit(likert_loyalty))
# Missing cases
data_missing <- colSums(is.na(likert_loyalty))
# Data summary
cbind(data_complete, data_missing)
fix(Custom)
names(Custom)
#[1] "Sat1"             "Sat2"             "Sat3"             "Sat4"
#[5] "CustSatMean"          "Custloyalty"      "INvestMore"       "SexOfSalesperson"
#make the dataset live
attach(Custom)
#Figure 7.3
modelf7_3=lm(Custloyalty ~ Sat1 + Sat2 + Sat3 + Sat4 + SexOfSalesperson)
modelf7_3
summary(modelf7_3)
coef(modelf7_3)
library(QuantPsyc)
lm.beta(modelf7_3)
#install.packages("trendyy")
#remotes::install_github("josiahparry/gtrendsR", "interest_refactor")
library(trendyy)
library(dplyr)
library(ggplot2)
# http://josiahparry.com/post/2019-05-25-introducing-trendyy/
# 1. Create Vector for search terms ----
#analytics <- c("paithani saree","banarasi saree","kantha saree","sambalpuri saree","khandua saree")
#analytics <- c("pasapali","bomkai","khandua silk")
#analytics  <- c("virat kohli","steve smith")
analytics <- c("salvos")
# 2. Get Query ----
analytics_trends <- trendy(analytics, from = "2015-01-01", to = Sys.Date())
# 3. Get Summary of trends ----
analytics_trends
# 4. Get Trend Data
get_interest(analytics_trends)
get_interest_country(analytics_trends)
# 5. Plot Trends
analytics_trends %>%
get_interest   %>%
ggplot(aes(date, hits, color = keyword)) +
#  geom_line() +
geom_smooth(span=0.5,se=FALSE)+
geom_point(alpha = .2) +
theme_minimal() +
theme(legend.position = "bottom") +
theme(plot.title = element_text(hjust = 0.5))+
labs(x = "",
y = "Relative Search Popularity",
title = "Google Search Popularity") +
#  ylim(0,10) +
ggsave("myplot.png")
# 6. Get Related Queries
analytics_trends %>%
get_related_queries() %>%
group_by(keyword) %>%
sample_n(20)
#https://www.salvationarmy.org.au/about-us/news-and-stories/media-newsroom/2014-doorknock-result/
e
.010.
library(worldmet)
install.packages('davidcarslaw/worldmet')
install.packages('https://cran.r-project.org/package=worldmet')
install.packages('http://cran.r-project.org/web/packages/worldmet')
install.packages('http://github.com/davidcarslaw/worldmet')
require(devtools)
install_github('davidcarslaw/worldmet')
library(worldmet)
getMeta(site = "melbourne")
dat <- importNOAA(code = "959360-99999", year = 2020)
head(dat)
dat <- importNOAA(code = "959360-99999", year = 2020,hourly = FALSE)
head(dat)
setwd("~/")
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
dat <- importNOAA(code = "959360-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather/file.csv")
head(dat)
#library(worldmet)
#getMeta(site = "melbourne")
dat <- importNOAA(code = "959360-99999", year = 2020,hourly = FALSE,path = "~/file.csv")
head(dat)
dat <- importNOAA(code = "959360-99999", year = 2020,hourly = FALSE,path = "~/")
head(dat)
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
importNOAA(code = "959360-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
library(AER)
data(959360-99999_2020)
load()
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
data(melbourne)
melbourne
load(file="melbourne.rds")
my.data <- readRDS("melbourne.rds")
View(my.data)
write.csv(my.data,file = "melbourne.csv")
#library(worldmet)
getMeta(site = "italy")
getMeta(site = "italy")
getMeta(site = "milan")
getMeta(site = "new york")
getMeta(site = "newyork")
getMeta(site = "sydney")
importNOAA(code = "947670-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
library(worldmet)
getMeta(site = "sydney")
getMeta(site = "melbourne")
getMeta(site = "sydney")
importNOAA(code = "947670-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
getMeta(site = "melbourne")
importNOAA(code = "948660-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
library(worldmet)
importNOAA(code = "959360-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
importNOAA(code = "948660-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
getMeta(site = "wuhei")
getMeta(site = "beijing")
getMeta(lat = 40, lon = 116.9)
getMeta(lat = 30.9, lon = 112)
getMeta(lat = 30.9756, lon = 112.2707)
getMeta(site = "wuhan")
source('~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather/weather.R')
getMeta(lat = 30.9756, lon = 112.2707)
importNOAA(code = "573780-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
getMeta(lat = 43, lon = 12)
importNOAA(code = "162040-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
getMeta(lat = 40, lon = -4)
importNOAA(code = "082220-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
getMeta(lat = 55.3781, lon = -3.436)
importNOAA(code = "032200-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
getMeta(lat = 21, lon = 78)
getMeta(site = "delhi")
getMeta(lat = 21, lon = 78)
importNOAA(code = "429370-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
getMeta(lat = 37.0902, lon = -95.7129)
importNOAA(code = "724519-93967", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
my.data <- readRDS("sydney.rds")
write.csv(my.data,file = "sydney.csv")
my.data <- readRDS("india.rds")
write.csv(my.data,file = "india.csv")
my.data <- readRDS("hubei.rds")
write.csv(my.data,file = "hubei.csv")
my.data <- readRDS("italy.rds")
write.csv(my.data,file = "italy.csv")
my.data <- readRDS("melbourne.rds")
write.csv(my.data,file = "melbourne.csv")
my.data <- readRDS("spain.rds")
write.csv(my.data,file = "spain.csv")
my.data <- readRDS("sydney.rds")
write.csv(my.data,file = "sydney.csv")
my.data <- readRDS("UK.rds")
write.csv(my.data,file = "UK.csv")
my.data <- readRDS("usa.rds")
write.csv(my.data,file = "usa.csv")
getMeta(lat = 43, lon = 12)
importNOAA(code = "161810-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
getMeta(site = "rome")
my.data <- readRDS("italy.rds")
write.csv(my.data,file = "italy.csv")
getMeta(site = "sydney")
importNOAA(code = "947680-99999", year = 2020,hourly = FALSE,path = "~/Google Drive/Data Analytics/Predictive HR Analytics/COVID-19/weather")
my.data <- readRDS("sydney.rds")
write.csv(my.data,file = "sydney.csv")
getMeta(lat = 37.0902, lon = -95.7129)
getMeta(lat = 43, lon = 12)
getMeta(lat = 30.9756, lon = 112.2707)
getMeta(site = "wuhei")
getMeta(site = "xiangong")
getMeta(lat = 30.9756, lon = 112.2707)
